{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdf1408b",
   "metadata": {},
   "source": [
    "# Model Training and Comparison for News Classification\n",
    "This notebook handles:\n",
    "- Loading preprocessed data\n",
    "- Implementing multiple unsupervised learning algorithms\n",
    "- Model training and parameter tuning\n",
    "- Performance comparison and evaluation\n",
    "- Results visualization and interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109250bd",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83a8c590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# PyTorch for deep learning and tensor operations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Machine Learning algorithms (keeping some for comparison)\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, calinski_harabasz_score, davies_bouldin_score,\n",
    "    adjusted_rand_score, normalized_mutual_info_score, homogeneity_score\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8943a1",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80475184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "âœ“ Loaded 1225 processed articles\n",
      "âœ“ Loaded TF-IDF features with 3 configurations\n",
      "âœ“ Loaded preprocessing summary\n",
      "\n",
      "Dataset Info:\n",
      "â€¢ Articles: 1225\n",
      "â€¢ TF-IDF configurations: ['basic', 'bigrams', 'trigrams']\n",
      "â€¢ Categories: 5\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "\n",
    "try:\n",
    "    # Load main dataframe\n",
    "    news_df = pd.read_csv('processed_news_data.csv')\n",
    "    print(f\"âœ“ Loaded {len(news_df)} processed articles\")\n",
    "    \n",
    "    # Load TF-IDF features\n",
    "    with open('tfidf_features.pkl', 'rb') as f:\n",
    "        tfidf_data = pickle.load(f)\n",
    "    \n",
    "    tfidf_matrices = tfidf_data['matrices']\n",
    "    tfidf_vectorizers = tfidf_data['vectorizers']\n",
    "    feature_names = tfidf_data['feature_names']\n",
    "    tfidf_configs = tfidf_data['configurations']\n",
    "    \n",
    "    print(f\"âœ“ Loaded TF-IDF features with {len(tfidf_matrices)} configurations\")\n",
    "    \n",
    "    # Load preprocessing summary\n",
    "    with open('preprocessing_summary.pkl', 'rb') as f:\n",
    "        preprocessing_summary = pickle.load(f)\n",
    "    \n",
    "    print(\"âœ“ Loaded preprocessing summary\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ Error loading preprocessed data: {e}\")\n",
    "    print(\"Please run the dataset_preprocessing.ipynb notebook first!\")\n",
    "    raise\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"â€¢ Articles: {len(news_df)}\")\n",
    "print(f\"â€¢ TF-IDF configurations: {list(tfidf_matrices.keys())}\")\n",
    "if 'label_text' in news_df.columns:\n",
    "    print(f\"â€¢ Categories: {news_df['label_text'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b88f3",
   "metadata": {},
   "source": [
    "## 4. PyTorch-based Model Trainer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca636ef",
   "metadata": {},
   "source": [
    "## 3. PyTorch K-Means Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "836db7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Models Defined!\n",
      "Classes available (following assignment requirements):\n",
      "1. PyTorchKMeans - Our custom K-means implementation (primary model)\n",
      "2. StochasticEmbeddingNetwork - Non-deterministic: z = f(x) + Îµ where Îµ ~ N(0, ÏƒÂ²)\n"
     ]
    }
   ],
   "source": [
    "class PyTorchKMeans(nn.Module):\n",
    "    \"\"\"\n",
    "    K-Means clustering implementation from scratch using PyTorch\n",
    "    Following neural network principles with gradient-based optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters, max_iter=300, tol=1e-4, init='k-means++'):\n",
    "        super(PyTorchKMeans, self).__init__()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.init = init\n",
    "        self.centroids = None\n",
    "        self.labels = None\n",
    "        self.inertia = None\n",
    "        self.n_iter = 0\n",
    "        \n",
    "    def _init_centroids(self, X):\n",
    "        \"\"\"\n",
    "        Initialize centroids using k-means++ algorithm or random initialization\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if self.init == 'k-means++':\n",
    "            centroids = torch.zeros(self.n_clusters, n_features, device=X.device)\n",
    "            \n",
    "            # Choose first centroid randomly\n",
    "            centroids[0] = X[torch.randint(0, n_samples, (1,))]\n",
    "            \n",
    "            # Choose remaining centroids using k-means++ algorithm\n",
    "            for i in range(1, self.n_clusters):\n",
    "                # Calculate distances from each point to nearest centroid\n",
    "                distances = torch.full((n_samples,), float('inf'), device=X.device)\n",
    "                \n",
    "                for j in range(i):\n",
    "                    dist_to_centroid = torch.sum((X - centroids[j]) ** 2, dim=1)\n",
    "                    distances = torch.minimum(distances, dist_to_centroid)\n",
    "                \n",
    "                # Choose next centroid with probability proportional to squared distance\n",
    "                probabilities = distances / torch.sum(distances)\n",
    "                cumsum = torch.cumsum(probabilities, dim=0)\n",
    "                r = torch.rand(1, device=X.device)\n",
    "                chosen_idx = torch.searchsorted(cumsum, r)\n",
    "                centroids[i] = X[chosen_idx]\n",
    "                \n",
    "        else:  # random initialization\n",
    "            indices = torch.randperm(n_samples, device=X.device)[:self.n_clusters]\n",
    "            centroids = X[indices].clone()\n",
    "            \n",
    "        return centroids\n",
    "    \n",
    "    def _assign_clusters(self, X, centroids):\n",
    "        \"\"\"\n",
    "        Assign each point to the nearest centroid\n",
    "        \"\"\"\n",
    "        distances = torch.cdist(X, centroids)  # Shape: (n_samples, n_clusters)\n",
    "        labels = torch.argmin(distances, dim=1)\n",
    "        return labels\n",
    "    \n",
    "    def _update_centroids(self, X, labels):\n",
    "        \"\"\"\n",
    "        Update centroids as the mean of assigned points\n",
    "        \"\"\"\n",
    "        centroids = torch.zeros(self.n_clusters, X.shape[1], device=X.device)\n",
    "        \n",
    "        for k in range(self.n_clusters):\n",
    "            mask = labels == k\n",
    "            if torch.sum(mask) > 0:\n",
    "                centroids[k] = torch.mean(X[mask], dim=0)\n",
    "            else:\n",
    "                # If no points assigned to cluster, reinitialize randomly\n",
    "                centroids[k] = X[torch.randint(0, X.shape[0], (1,))]\n",
    "        \n",
    "        return centroids\n",
    "    \n",
    "    def _calculate_inertia(self, X, labels, centroids):\n",
    "        \"\"\"\n",
    "        Calculate within-cluster sum of squared distances (inertia)\n",
    "        \"\"\"\n",
    "        inertia = 0.0\n",
    "        for k in range(self.n_clusters):\n",
    "            mask = labels == k\n",
    "            if torch.sum(mask) > 0:\n",
    "                cluster_points = X[mask]\n",
    "                centroid = centroids[k]\n",
    "                inertia += torch.sum((cluster_points - centroid) ** 2)\n",
    "        return inertia\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit K-means clustering to data X\n",
    "        \"\"\"\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            X = X.to(device)\n",
    "        \n",
    "        # Initialize centroids\n",
    "        centroids = self._init_centroids(X)\n",
    "        prev_inertia = float('inf')\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # Assign points to clusters\n",
    "            labels = self._assign_clusters(X, centroids)\n",
    "            \n",
    "            # Update centroids\n",
    "            new_centroids = self._update_centroids(X, labels)\n",
    "            \n",
    "            # Calculate inertia\n",
    "            inertia = self._calculate_inertia(X, labels, new_centroids)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if abs(prev_inertia - inertia) < self.tol:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "                \n",
    "            centroids = new_centroids\n",
    "            prev_inertia = inertia\n",
    "            self.n_iter = iteration + 1\n",
    "        \n",
    "        self.centroids = centroids\n",
    "        self.labels = labels\n",
    "        self.inertia = inertia\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict cluster labels for new data\n",
    "        \"\"\"\n",
    "        if self.centroids is None:\n",
    "            raise ValueError(\"Model must be fitted before prediction\")\n",
    "        \n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            X = X.to(device)\n",
    "        \n",
    "        return self._assign_clusters(X, self.centroids)\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"\n",
    "        Fit the model and return cluster labels\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.labels.cpu().numpy()\n",
    "    \n",
    "    def get_centroids(self):\n",
    "        \"\"\"\n",
    "        Get the centroids as numpy array\n",
    "        \"\"\"\n",
    "        if self.centroids is None:\n",
    "            return None\n",
    "        return self.centroids.cpu().numpy()\n",
    "\n",
    "\n",
    "class StochasticEmbeddingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Non-Deterministic Stochastic Embedding Network for Clustering\n",
    "    Based on assignment requirement: z = f(x) + Îµ where Îµ ~ N(0, ÏƒÂ²)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, n_clusters, hidden_dim=128, embedding_dim=64, noise_scale=0.1):\n",
    "        super(StochasticEmbeddingNetwork, self).__init__()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.noise_scale = noise_scale\n",
    "        \n",
    "        # Encoder network for deterministic component f(x)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, embedding_dim),\n",
    "        )\n",
    "        \n",
    "        # Variance network to predict noise scale ÏƒÂ²\n",
    "        self.variance_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, embedding_dim),\n",
    "            nn.Softplus()  # Ensures positive variance\n",
    "        )\n",
    "        \n",
    "        # Learnable cluster centers with uncertainty\n",
    "        self.cluster_centers_mu = nn.Parameter(torch.randn(n_clusters, embedding_dim) * 0.1)\n",
    "        self.cluster_centers_logvar = nn.Parameter(torch.ones(n_clusters, embedding_dim) * -2)\n",
    "        \n",
    "        # Temperature parameter for soft assignments\n",
    "        self.temperature = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: sample from N(mu, exp(logvar))\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x, sample=True):\n",
    "        \"\"\"\n",
    "        Forward pass with stochastic embeddings\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Deterministic embedding component f(x)\n",
    "        deterministic_embedding = self.encoder(x)\n",
    "        \n",
    "        if sample:\n",
    "            # Predict input-dependent noise variance\n",
    "            embedding_logvar = self.variance_net(x)\n",
    "            \n",
    "            # Stochastic embedding: z = f(x) + Îµ where Îµ ~ N(0, ÏƒÂ²)\n",
    "            stochastic_embedding = self.reparameterize(deterministic_embedding, embedding_logvar)\n",
    "        else:\n",
    "            stochastic_embedding = deterministic_embedding\n",
    "            embedding_logvar = torch.zeros_like(deterministic_embedding)\n",
    "        \n",
    "        # Sample cluster centers from their distributions\n",
    "        if sample and self.training:\n",
    "            cluster_centers = self.reparameterize(\n",
    "                self.cluster_centers_mu, \n",
    "                self.cluster_centers_logvar\n",
    "            )\n",
    "        else:\n",
    "            cluster_centers = self.cluster_centers_mu\n",
    "        \n",
    "        # Calculate distances to cluster centers\n",
    "        distances = torch.cdist(stochastic_embedding, cluster_centers)\n",
    "        \n",
    "        # Temperature-scaled soft assignments\n",
    "        probabilities = F.softmax(-distances / self.temperature, dim=1)\n",
    "        \n",
    "        return {\n",
    "            'embeddings': stochastic_embedding,\n",
    "            'deterministic_embeddings': deterministic_embedding,\n",
    "            'embedding_logvar': embedding_logvar,\n",
    "            'probabilities': probabilities,\n",
    "            'distances': distances,\n",
    "            'cluster_centers': cluster_centers,\n",
    "            'uncertainty': torch.exp(0.5 * embedding_logvar).mean(dim=1)  # Per-sample uncertainty\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Neural Network Models Defined!\")\n",
    "print(\"Classes available (following assignment requirements):\")\n",
    "print(\"1. PyTorchKMeans - Our custom K-means implementation (primary model)\")\n",
    "print(\"2. StochasticEmbeddingNetwork - Non-deterministic: z = f(x) + Îµ where Îµ ~ N(0, ÏƒÂ²)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchModelTrainer:\n",
    "    \"\"\"\n",
    "    Model trainer using PyTorch implementations built from scratch\n",
    "    \"\"\"\n",
    "    def __init__(self, data_matrix, true_labels=None):\n",
    "        self.data_matrix = data_matrix\n",
    "        self.true_labels = true_labels\n",
    "        self.models = {}\n",
    "        self.predictions = {}\n",
    "        self.training_times = {}\n",
    "        self.evaluation_metrics = {}\n",
    "        \n",
    "        # Convert data to PyTorch tensor format\n",
    "        if hasattr(data_matrix, 'toarray'):\n",
    "            self.torch_data = torch.tensor(data_matrix.toarray(), dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            self.torch_data = torch.tensor(data_matrix, dtype=torch.float32, device=device)\n",
    "        \n",
    "        print(f\"Data converted to PyTorch tensor: {self.torch_data.shape}\")\n",
    "    \n",
    "    def train_pytorch_kmeans(self, n_clusters_range=None, init_method='k-means++'):\n",
    "        \"\"\"\n",
    "        Train PyTorch K-Means implementation built from scratch\n",
    "        \"\"\"\n",
    "        if n_clusters_range is None:\n",
    "            n_clusters_range = range(2, 11)\n",
    "        \n",
    "        print(f\"Training PyTorch K-Means from scratch (init: {init_method})...\")\n",
    "        \n",
    "        best_score = -1\n",
    "        best_k = 2\n",
    "        \n",
    "        for k in n_clusters_range:\n",
    "            print(f\"  Training K-Means with K={k}...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Initialize PyTorch K-Means model\n",
    "            model = PyTorchKMeans(n_clusters=k, max_iter=300, init=init_method)\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(self.torch_data)\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = model.labels.cpu().numpy()\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate silhouette score\n",
    "            if len(np.unique(predictions)) > 1:\n",
    "                sil_score = silhouette_score(self.data_matrix, predictions)\n",
    "                \n",
    "                # Store best model\n",
    "                if sil_score > best_score:\n",
    "                    best_score = sil_score\n",
    "                    best_k = k\n",
    "                    self.models['pytorch_kmeans'] = model\n",
    "                    self.predictions['pytorch_kmeans'] = predictions\n",
    "                    self.training_times['pytorch_kmeans'] = training_time\n",
    "                \n",
    "                print(f\"    K={k}: Silhouette Score = {sil_score:.3f}, Inertia = {model.inertia:.2f}, Time = {training_time:.2f}s, Iterations = {model.n_iter}\")\n",
    "            else:\n",
    "                print(f\"    K={k}: Failed to create multiple clusters\")\n",
    "        \n",
    "        print(f\"âœ“ Best PyTorch K-Means: K={best_k}, Silhouette Score = {best_score:.3f}\")\n",
    "        return best_k, best_score\n",
    "    \n",
    "    def train_stochastic_embedding(self, n_clusters_range=None, hidden_dim=128, embedding_dim=64, epochs=150, noise_scale=0.1):\n",
    "        \"\"\"\n",
    "        Train Stochastic Embedding Network (Assignment Requirement)\n",
    "        \"\"\"\n",
    "        if n_clusters_range is None:\n",
    "            n_clusters_range = range(2, 8)\n",
    "        \n",
    "        print(f\"Training Stochastic Embedding Network (Non-Deterministic)...\")\n",
    "        \n",
    "        best_score = -1\n",
    "        best_k = 2\n",
    "        input_dim = self.torch_data.shape[1]\n",
    "        \n",
    "        for k in n_clusters_range:\n",
    "            print(f\"  Training Stochastic Embedding with K={k}...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Initialize Stochastic Embedding model\n",
    "            model = StochasticEmbeddingNetwork(\n",
    "                input_dim=input_dim,\n",
    "                n_clusters=k,\n",
    "                hidden_dim=hidden_dim,\n",
    "                embedding_dim=embedding_dim,\n",
    "                noise_scale=noise_scale\n",
    "            ).to(device)\n",
    "            \n",
    "            # Train the model\n",
    "            self._train_stochastic_model(model, epochs=epochs, model_name='stochastic_embedding')\n",
    "            \n",
    "            # Get predictions with multiple samples for stability\n",
    "            predictions = self._predict_stochastic(model, n_samples=10)\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate silhouette score\n",
    "            if len(np.unique(predictions)) > 1:\n",
    "                sil_score = silhouette_score(self.data_matrix, predictions)\n",
    "                \n",
    "                if sil_score > best_score:\n",
    "                    best_score = sil_score\n",
    "                    best_k = k\n",
    "                    self.models['stochastic_embedding'] = model\n",
    "                    self.predictions['stochastic_embedding'] = predictions\n",
    "                    self.training_times['stochastic_embedding'] = training_time\n",
    "                \n",
    "                print(f\"    K={k}: Silhouette Score = {sil_score:.3f}, Time = {training_time:.2f}s\")\n",
    "            else:\n",
    "                print(f\"    K={k}: Failed to create multiple clusters\")\n",
    "        \n",
    "        print(f\"âœ“ Best Stochastic Embedding: K={best_k}, Silhouette Score = {best_score:.3f}\")\n",
    "        return best_k, best_score\n",
    "    \n",
    "\n",
    "    \n",
    "    def evaluate_all_models(self):\n",
    "        \"\"\"\n",
    "        Evaluate all trained models with comprehensive metrics\n",
    "        \"\"\"\n",
    "        print(\"\\nEvaluating all models...\")\n",
    "        \n",
    "        for model_name, predictions in self.predictions.items():\n",
    "            metrics = {}\n",
    "            \n",
    "            # Unsupervised metrics\n",
    "            if len(np.unique(predictions)) > 1:\n",
    "                try:\n",
    "                    metrics['silhouette_score'] = silhouette_score(self.data_matrix, predictions)\n",
    "                    data_for_metrics = self.data_matrix.toarray() if hasattr(self.data_matrix, 'toarray') else self.data_matrix\n",
    "                    metrics['calinski_harabasz_score'] = calinski_harabasz_score(data_for_metrics, predictions)\n",
    "                    metrics['davies_bouldin_score'] = davies_bouldin_score(data_for_metrics, predictions)\n",
    "                except Exception as e:\n",
    "                    print(f\"    Warning: Could not calculate some metrics for {model_name}: {e}\")\n",
    "            \n",
    "            # Supervised metrics (if true labels available)\n",
    "            if self.true_labels is not None:\n",
    "                try:\n",
    "                    metrics['adjusted_rand_score'] = adjusted_rand_score(self.true_labels, predictions)\n",
    "                    metrics['normalized_mutual_info_score'] = normalized_mutual_info_score(self.true_labels, predictions)\n",
    "                    metrics['homogeneity_score'] = homogeneity_score(self.true_labels, predictions)\n",
    "                except Exception as e:\n",
    "                    print(f\"    Warning: Could not calculate supervised metrics for {model_name}: {e}\")\n",
    "            \n",
    "            metrics['n_clusters'] = len(np.unique(predictions))\n",
    "            metrics['training_time'] = self.training_times.get(model_name, 0)\n",
    "            \n",
    "            # Add PyTorch-specific metrics\n",
    "            if model_name == 'pytorch_kmeans':\n",
    "                model = self.models[model_name]\n",
    "                if hasattr(model, 'inertia') and model.inertia is not None:\n",
    "                    metrics['inertia'] = float(model.inertia)\n",
    "                if hasattr(model, 'n_iter'):\n",
    "                    metrics['iterations'] = model.n_iter\n",
    "            \n",
    "            self.evaluation_metrics[model_name] = metrics\n",
    "        \n",
    "        return self.evaluation_metrics\n",
    "    \n",
    "    def get_cluster_analysis(self):\n",
    "        \"\"\"\n",
    "        Detailed analysis of PyTorch K-means clusters\n",
    "        \"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        if 'pytorch_kmeans' in self.models and 'pytorch_kmeans' in self.predictions:\n",
    "            model = self.models['pytorch_kmeans']\n",
    "            predictions = self.predictions['pytorch_kmeans']\n",
    "            \n",
    "            cluster_info = {}\n",
    "            cluster_info['n_clusters'] = len(np.unique(predictions))\n",
    "            cluster_info['cluster_sizes'] = [np.sum(predictions == k) for k in np.unique(predictions)]\n",
    "            \n",
    "            if hasattr(model, 'get_centroids'):\n",
    "                centroids = model.get_centroids()\n",
    "                if centroids is not None:\n",
    "                    cluster_info['centroids_shape'] = centroids.shape\n",
    "                    # Calculate centroid distances\n",
    "                    centroid_distances = []\n",
    "                    for i in range(len(centroids)):\n",
    "                        for j in range(i+1, len(centroids)):\n",
    "                            dist = np.linalg.norm(centroids[i] - centroids[j])\n",
    "                            centroid_distances.append(dist)\n",
    "                    cluster_info['avg_centroid_distance'] = np.mean(centroid_distances) if centroid_distances else 0\n",
    "            \n",
    "            analysis['pytorch_kmeans'] = cluster_info\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _train_stochastic_model(self, model, epochs=150, batch_size=256, lr=0.001, model_name='stochastic'):\n",
    "        \"\"\"\n",
    "        Training procedure for Stochastic Embedding Network\n",
    "        \"\"\"\n",
    "        dataset = TensorDataset(self.torch_data)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            for batch_data in dataloader:\n",
    "                batch_X = batch_data[0]\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass with stochastic sampling\n",
    "                outputs = model(batch_X, sample=True)\n",
    "                \n",
    "                # Clustering loss: minimize distance to assigned clusters\n",
    "                cluster_assignments = torch.argmax(outputs['probabilities'], dim=1)\n",
    "                cluster_loss = 0.0\n",
    "                \n",
    "                for k in range(model.n_clusters):\n",
    "                    mask = cluster_assignments == k\n",
    "                    if mask.sum() > 0:\n",
    "                        cluster_points = outputs['embeddings'][mask]\n",
    "                        centroid = outputs['cluster_centers'][k]\n",
    "                        cluster_loss += torch.mean((cluster_points - centroid) ** 2)\n",
    "                \n",
    "                # Regularization: encourage diversity in cluster assignments\n",
    "                avg_prob = torch.mean(outputs['probabilities'], dim=0)\n",
    "                entropy_reg = -torch.sum(avg_prob * torch.log(avg_prob + 1e-8))\n",
    "                \n",
    "                # Uncertainty regularization: penalize excessive uncertainty\n",
    "                uncertainty_reg = torch.mean(outputs['uncertainty'])\n",
    "                \n",
    "                # Total loss\n",
    "                loss = cluster_loss - 0.1 * entropy_reg + 0.01 * uncertainty_reg\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 30 == 0:\n",
    "                print(f\"    Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "\n",
    "    \n",
    "    def _predict_stochastic(self, model, n_samples=10):\n",
    "        \"\"\"\n",
    "        Predict with multiple stochastic samples for stability\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(n_samples):\n",
    "                outputs = model(self.torch_data, sample=True)\n",
    "                predictions = torch.argmax(outputs['probabilities'], dim=1).cpu().numpy()\n",
    "                all_predictions.append(predictions)\n",
    "        \n",
    "        # Ensemble prediction: majority vote\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        final_predictions = []\n",
    "        \n",
    "        for i in range(all_predictions.shape[1]):\n",
    "            votes = all_predictions[:, i]\n",
    "            final_predictions.append(np.bincount(votes).argmax())\n",
    "        \n",
    "        return np.array(final_predictions)\n",
    "    \n",
    "\n",
    "    \n",
    "    def get_uncertainty_analysis(self):\n",
    "        \"\"\"\n",
    "        Analyze uncertainty in non-deterministic models\n",
    "        \"\"\"\n",
    "        uncertainty_analysis = {}\n",
    "        \n",
    "        # Analyze stochastic embedding uncertainty\n",
    "        if 'stochastic_embedding' in self.models:\n",
    "            model = self.models['stochastic_embedding']\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(self.torch_data, sample=False)\n",
    "                uncertainty = outputs['uncertainty'].cpu().numpy()\n",
    "                \n",
    "                uncertainty_analysis['stochastic_embedding'] = {\n",
    "                    'mean_uncertainty': np.mean(uncertainty),\n",
    "                    'std_uncertainty': np.std(uncertainty),\n",
    "                    'max_uncertainty': np.max(uncertainty),\n",
    "                    'min_uncertainty': np.min(uncertainty)\n",
    "                }\n",
    "        \n",
    "\n",
    "        \n",
    "        return uncertainty_analysis\n",
    "\n",
    "\n",
    "print(\"PyTorchModelTrainer class defined!\")\n",
    "print(\"Features (Assignment Compliance):\")\n",
    "print(\"- âœ… Our custom PyTorch K-means implementation\") \n",
    "print(\"- âœ… Stochastic Embedding Network with reparameterization trick\")\n",
    "print(\"- âœ… Uncertainty quantification\")\n",
    "print(\"- âœ… Multiple evaluation metrics from assignment\")\n",
    "print(\"- âœ… Comparative analysis between deterministic and stochastic approaches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be298a3d",
   "metadata": {},
   "source": [
    "## 5. PyTorch Model Training from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose TF-IDF configuration for training\n",
    "config_choice = 'bigrams'  # Change this to 'basic' or 'trigrams' if desired\n",
    "\n",
    "print(f\"Using '{config_choice}' TF-IDF configuration for PyTorch model training\")\n",
    "data_matrix = tfidf_matrices[config_choice]\n",
    "print(f\"Data matrix shape: {data_matrix.shape}\")\n",
    "\n",
    "# Prepare true labels if available\n",
    "true_labels = None\n",
    "if 'label_text' in news_df.columns:\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    true_labels = label_encoder.fit_transform(news_df['label_text'])\n",
    "    print(f\"True labels available: {len(np.unique(true_labels))} categories\")\n",
    "    print(f\"Categories: {list(label_encoder.classes_)}\")\n",
    "\n",
    "# Initialize PyTorch trainer\n",
    "pytorch_trainer = PyTorchModelTrainer(data_matrix, true_labels)\n",
    "print(\"\\nPyTorchModelTrainer initialized!\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Data tensor shape: {pytorch_trainer.torch_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc8ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PyTorch K-Means from scratch\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING PYTORCH K-MEANS FROM SCRATCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train standard PyTorch K-Means with k-means++ initialization\n",
    "best_k_standard, best_score_standard = pytorch_trainer.train_pytorch_kmeans(\n",
    "    n_clusters_range=range(2, 12), \n",
    "    init_method='k-means++'\n",
    ")\n",
    "\n",
    "print(f\"\\nBest PyTorch K-Means Configuration:\")\n",
    "print(f\"  - K: {best_k_standard}\")\n",
    "print(f\"  - Silhouette Score: {best_score_standard:.4f}\")\n",
    "print(f\"  - Initialization: k-means++\")\n",
    "\n",
    "# Also try random initialization for comparison\n",
    "print(f\"\\nTraining with random initialization for comparison...\")\n",
    "pytorch_trainer_random = PyTorchModelTrainer(data_matrix, true_labels)\n",
    "best_k_random, best_score_random = pytorch_trainer_random.train_pytorch_kmeans(\n",
    "    n_clusters_range=range(2, 8), \n",
    "    init_method='random'\n",
    ")\n",
    "\n",
    "print(f\"\\nComparison of initialization methods:\")\n",
    "print(f\"  K-means++: K={best_k_standard}, Score={best_score_standard:.4f}\")\n",
    "print(f\"  Random:    K={best_k_random}, Score={best_score_random:.4f}\")\n",
    "\n",
    "# Use the better performing model\n",
    "if best_score_random > best_score_standard:\n",
    "    print(\"Using random initialization model (better performance)\")\n",
    "    pytorch_trainer = pytorch_trainer_random\n",
    "else:\n",
    "    print(\"Using k-means++ initialization model (better performance)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e64043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Stochastic Embedding Network (Assignment Requirement)\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING STOCHASTIC EMBEDDING NETWORK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train Stochastic Embedding Network\n",
    "print(\"\\nðŸŽ² STOCHASTIC EMBEDDING NETWORK (z = f(x) + Îµ)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "best_k_stochastic, best_score_stochastic = pytorch_trainer.train_stochastic_embedding(\n",
    "    n_clusters_range=range(2, 8),\n",
    "    hidden_dim=256,\n",
    "    embedding_dim=128,\n",
    "    epochs=200,\n",
    "    noise_scale=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\nBest Stochastic Embedding Configuration:\")\n",
    "print(f\"  - K: {best_k_stochastic}\")\n",
    "print(f\"  - Silhouette Score: {best_score_stochastic:.4f}\")\n",
    "print(f\"  - Architecture: Non-deterministic with input-dependent noise\")\n",
    "print(f\"  - Stochastic formula: z = f(x) + Îµ where Îµ ~ N(0, ÏƒÂ²(x))\")\n",
    "\n",
    "# Compare with our PyTorch K-means\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"STOCHASTIC vs OUR DETERMINISTIC K-MEANS COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"ðŸŽ² Stochastic Embedding:     Score = {best_score_stochastic:.4f}\")\n",
    "print(f\"ðŸ”§ Our PyTorch K-means:      Score = {best_score_standard:.4f}\")\n",
    "\n",
    "if best_score_stochastic > best_score_standard:\n",
    "    improvement = ((best_score_stochastic - best_score_standard) / best_score_standard) * 100\n",
    "    print(f\"\\nâœ… Stochastic method performs {improvement:.1f}% better!\")\n",
    "else:\n",
    "    improvement = ((best_score_standard - best_score_stochastic) / best_score_stochastic) * 100\n",
    "    print(f\"\\nðŸ”§ Our deterministic K-means performs {improvement:.1f}% better!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83be6463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66bc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty Analysis for Non-Deterministic Models\n",
    "print(\"=\"*70)\n",
    "print(\"UNCERTAINTY QUANTIFICATION ANALYSIS (Assignment Requirement)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get uncertainty analysis\n",
    "uncertainty_analysis = pytorch_trainer.get_uncertainty_analysis()\n",
    "\n",
    "if uncertainty_analysis:\n",
    "    print(\"\\nðŸŽ² UNCERTAINTY METRICS:\")\n",
    "    \n",
    "    for model_name, metrics in uncertainty_analysis.items():\n",
    "        print(f\"\\n{model_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"  Mean Uncertainty: {metrics['mean_uncertainty']:.4f}\")\n",
    "        print(f\"  Std Uncertainty:  {metrics['std_uncertainty']:.4f}\")\n",
    "        print(f\"  Max Uncertainty:  {metrics['max_uncertainty']:.4f}\")\n",
    "        print(f\"  Min Uncertainty:  {metrics['min_uncertainty']:.4f}\")\n",
    "    \n",
    "    # Compare uncertainty between models\n",
    "    if len(uncertainty_analysis) >= 2:\n",
    "        models = list(uncertainty_analysis.keys())\n",
    "        mean_uncertainties = [uncertainty_analysis[m]['mean_uncertainty'] for m in models]\n",
    "        \n",
    "        print(f\"\\nUncertainty Comparison:\")\n",
    "        for i, model in enumerate(models):\n",
    "            print(f\"  {model}: {mean_uncertainties[i]:.4f}\")\n",
    "        \n",
    "        most_uncertain = models[np.argmax(mean_uncertainties)]\n",
    "        least_uncertain = models[np.argmin(mean_uncertainties)]\n",
    "        print(f\"\\n  Most uncertain: {most_uncertain}\")\n",
    "        print(f\"  Least uncertain: {least_uncertain}\")\n",
    "\n",
    "# Stability Analysis: Multiple Runs\n",
    "print(f\"\\nðŸ”„ STABILITY ANALYSIS:\")\n",
    "print(\"Testing model consistency across multiple runs...\")\n",
    "\n",
    "stability_results = {}\n",
    "\n",
    "# Test stochastic embedding multiple times\n",
    "if 'stochastic_embedding' in pytorch_trainer.models:\n",
    "    model = pytorch_trainer.models['stochastic_embedding']\n",
    "    \n",
    "    # Get predictions from multiple runs\n",
    "    predictions_list = []\n",
    "    for run in range(5):\n",
    "        preds = pytorch_trainer._predict_stochastic(model, n_samples=5)\n",
    "        predictions_list.append(preds)\n",
    "    \n",
    "    # Calculate stability (average ARI between runs)\n",
    "    ari_scores = []\n",
    "    for i in range(len(predictions_list)):\n",
    "        for j in range(i+1, len(predictions_list)):\n",
    "            ari = adjusted_rand_score(predictions_list[i], predictions_list[j])\n",
    "            ari_scores.append(ari)\n",
    "    \n",
    "    stability_results['stochastic_embedding'] = {\n",
    "        'mean_ari': np.mean(ari_scores),\n",
    "        'std_ari': np.std(ari_scores),\n",
    "        'min_ari': np.min(ari_scores),\n",
    "        'max_ari': np.max(ari_scores)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nStochastic Embedding Stability:\")\n",
    "    print(f\"  Mean ARI between runs: {np.mean(ari_scores):.4f}\")\n",
    "    print(f\"  Std ARI: {np.std(ari_scores):.4f}\")\n",
    "    print(f\"  Stability range: [{np.min(ari_scores):.4f}, {np.max(ari_scores):.4f}]\")\n",
    "\n",
    "# Detailed Model Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"DETAILED MODEL ARCHITECTURE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get cluster analysis for all models\n",
    "cluster_analysis = pytorch_trainer.get_cluster_analysis()\n",
    "\n",
    "for model_name, analysis in cluster_analysis.items():\n",
    "    print(f\"\\n{model_name.upper().replace('_', ' ')} ANALYSIS:\")\n",
    "    print(f\"  Number of clusters: {analysis['n_clusters']}\")\n",
    "    print(f\"  Cluster sizes: {analysis['cluster_sizes']}\")\n",
    "    \n",
    "    if 'avg_centroid_distance' in analysis:\n",
    "        print(f\"  Average centroid distance: {analysis['avg_centroid_distance']:.4f}\")\n",
    "    \n",
    "    if 'centroids_shape' in analysis:\n",
    "        print(f\"  Centroids shape: {analysis['centroids_shape']}\")\n",
    "\n",
    "# Model Agreement Analysis\n",
    "print(f\"\\nðŸ“Š MODEL AGREEMENT ANALYSIS:\")\n",
    "\n",
    "model_names = list(pytorch_trainer.predictions.keys())\n",
    "if len(model_names) >= 2:\n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(i+1, len(model_names)):\n",
    "            model1, model2 = model_names[i], model_names[j]\n",
    "            preds1 = pytorch_trainer.predictions[model1]\n",
    "            preds2 = pytorch_trainer.predictions[model2]\n",
    "            \n",
    "            agreement = adjusted_rand_score(preds1, preds2)\n",
    "            print(f\"  {model1} â†” {model2}: ARI = {agreement:.4f}\")\n",
    "\n",
    "# Stochastic vs Our K-means Comparison\n",
    "if 'stochastic_embedding' in pytorch_trainer.predictions and 'pytorch_kmeans' in pytorch_trainer.predictions:\n",
    "    print(f\"\\nðŸŽ² STOCHASTIC vs OUR K-MEANS:\")\n",
    "    preds_stochastic = pytorch_trainer.predictions['stochastic_embedding']\n",
    "    preds_kmeans = pytorch_trainer.predictions['pytorch_kmeans']\n",
    "    agreement = adjusted_rand_score(preds_stochastic, preds_kmeans)\n",
    "    print(f\"  Stochastic Embedding vs Our PyTorch K-means: ARI = {agreement:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc869c3",
   "metadata": {},
   "source": [
    "## 6. PyTorch Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f96f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all PyTorch models\n",
    "evaluation_results = pytorch_trainer.evaluate_all_models()\n",
    "\n",
    "# Create evaluation DataFrame\n",
    "eval_df = pd.DataFrame(evaluation_results).T\n",
    "eval_df = eval_df.round(4)\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"NON-DETERMINISTIC UNSUPERVISED NEURAL NETWORK EVALUATION\")\n",
    "print(\"(Following Assignment Requirements - Due Sept 14, 2025)\")\n",
    "print(\"=\"*90)\n",
    "display(eval_df)\n",
    "\n",
    "# Print detailed summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'silhouette_score' in eval_df.columns:\n",
    "    best_silhouette = eval_df['silhouette_score'].idxmax()\n",
    "    print(f\"ðŸ† Best Silhouette Score: {best_silhouette.upper()} ({eval_df.loc[best_silhouette, 'silhouette_score']:.4f})\")\n",
    "\n",
    "if true_labels is not None and 'adjusted_rand_score' in eval_df.columns:\n",
    "    best_ari = eval_df['adjusted_rand_score'].idxmax()\n",
    "    print(f\"ðŸŽ¯ Best Adjusted Rand Index: {best_ari.upper()} ({eval_df.loc[best_ari, 'adjusted_rand_score']:.4f})\")\n",
    "\n",
    "if 'training_time' in eval_df.columns:\n",
    "    fastest = eval_df['training_time'].idxmin()\n",
    "    print(f\"âš¡ Fastest Training: {fastest.upper()} ({eval_df.loc[fastest, 'training_time']:.2f}s)\")\n",
    "\n",
    "# Model Performance Summary\n",
    "available_models = ['pytorch_kmeans', 'stochastic_embedding']\n",
    "available_models = [m for m in available_models if m in eval_df.index]\n",
    "\n",
    "if available_models:\n",
    "    print(f\"\\nModel Performance Summary:\")\n",
    "    for model in available_models:\n",
    "        sil_score = eval_df.loc[model, 'silhouette_score'] if 'silhouette_score' in eval_df.columns else 'N/A'\n",
    "        n_clusters = eval_df.loc[model, 'n_clusters'] if 'n_clusters' in eval_df.columns else 'N/A'\n",
    "        time = eval_df.loc[model, 'training_time'] if 'training_time' in eval_df.columns else 'N/A'\n",
    "        \n",
    "        print(f\"  {model.replace('_', ' ').title()}:\")\n",
    "        print(f\"    - Clusters: {n_clusters}\")\n",
    "        print(f\"    - Silhouette: {sil_score}\")\n",
    "        print(f\"    - Time: {time}s\")\n",
    "        \n",
    "        if model == 'pytorch_kmeans':\n",
    "            iterations = eval_df.loc[model, 'iterations'] if 'iterations' in eval_df.columns else 'N/A'\n",
    "            if iterations != 'N/A':\n",
    "                print(f\"    - Iterations: {iterations}\")\n",
    "\n",
    "# Compare our two models directly\n",
    "if 'pytorch_kmeans' in eval_df.index and 'stochastic_embedding' in eval_df.index:\n",
    "    print(f\"\\nDirect Comparison: Our K-means vs Stochastic Embedding\")\n",
    "    \n",
    "    kmeans_sil = eval_df.loc['pytorch_kmeans', 'silhouette_score'] if 'silhouette_score' in eval_df.columns else 0\n",
    "    stoch_sil = eval_df.loc['stochastic_embedding', 'silhouette_score'] if 'silhouette_score' in eval_df.columns else 0\n",
    "    \n",
    "    print(f\"  Silhouette Score:\")\n",
    "    print(f\"    - Our PyTorch K-means: {kmeans_sil:.4f}\")\n",
    "    print(f\"    - Stochastic Embedding: {stoch_sil:.4f}\")\n",
    "    \n",
    "    if kmeans_sil > stoch_sil:\n",
    "        print(f\"  âœ… Our K-means implementation performs better!\")\n",
    "    else:\n",
    "        print(f\"  ðŸŽ² Stochastic embedding performs better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7a7ce",
   "metadata": {},
   "source": [
    "## 7. PyTorch Models Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af18d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations for our two models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Silhouette scores comparison\n",
    "if 'silhouette_score' in eval_df.columns:\n",
    "    silhouette_scores = eval_df['silhouette_score'].dropna()\n",
    "    colors = ['blue' if idx == 'pytorch_kmeans' else 'red' for idx in silhouette_scores.index]\n",
    "    bars = axes[0, 0].bar(silhouette_scores.index, silhouette_scores.values, color=colors)\n",
    "    axes[0, 0].set_title('Silhouette Score Comparison\\n(Blue: Our K-means, Red: Stochastic)', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Silhouette Score')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, silhouette_scores.values):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                       f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Training time comparison\n",
    "if 'training_time' in eval_df.columns:\n",
    "    training_times = eval_df['training_time'].dropna()\n",
    "    colors = ['blue' if idx == 'pytorch_kmeans' else 'red' for idx in training_times.index]\n",
    "    bars = axes[0, 1].bar(training_times.index, training_times.values, color=colors)\n",
    "    axes[0, 1].set_title('Training Time Comparison\\n(Blue: Our K-means, Red: Stochastic)', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Time (seconds)')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Supervised metrics (if available)\n",
    "if true_labels is not None and 'adjusted_rand_score' in eval_df.columns:\n",
    "    ari_scores = eval_df['adjusted_rand_score'].dropna()\n",
    "    colors = ['blue' if idx == 'pytorch_kmeans' else 'red' for idx in ari_scores.index]\n",
    "    bars = axes[1, 0].bar(ari_scores.index, ari_scores.values, color=colors)\n",
    "    axes[1, 0].set_title('Adjusted Rand Index\\n(Blue: Our K-means, Red: Stochastic)', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('ARI Score')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Number of clusters found\n",
    "if 'n_clusters' in eval_df.columns:\n",
    "    n_clusters_data = eval_df['n_clusters'].dropna()\n",
    "    colors = ['blue' if idx == 'pytorch_kmeans' else 'red' for idx in n_clusters_data.index]\n",
    "    bars = axes[1, 1].bar(n_clusters_data.index, n_clusters_data.values, color=colors)\n",
    "    axes[1, 1].set_title('Number of Clusters Found\\n(Blue: Our K-means, Red: Stochastic)', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Number of Clusters')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, n_clusters_data.values):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                       f'{int(value)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pytorch_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Model comparison visualizations saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24cd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction for visualization\n",
    "print(\"Creating 2D visualization using PCA and t-SNE...\")\n",
    "\n",
    "# Use the same data matrix that pytorch_trainer uses\n",
    "viz_data = data_matrix.toarray() if hasattr(data_matrix, 'toarray') else data_matrix\n",
    "\n",
    "# PCA reduction\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_features = pca.fit_transform(viz_data)\n",
    "\n",
    "# t-SNE reduction (on PCA-reduced data for speed)\n",
    "pca_50 = PCA(n_components=50, random_state=42)\n",
    "pca_50_features = pca_50.fit_transform(viz_data)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "tsne_features = tsne.fit_transform(pca_50_features)\n",
    "\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "print(\"âœ“ Dimensionality reduction completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2290f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our two models side by side\n",
    "if 'pytorch_kmeans' in pytorch_trainer.predictions and 'stochastic_embedding' in pytorch_trainer.predictions:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Our PyTorch K-Means\n",
    "    pytorch_preds = pytorch_trainer.predictions['pytorch_kmeans']\n",
    "    scatter1 = axes[0].scatter(pca_features[:, 0], pca_features[:, 1], \n",
    "                              c=pytorch_preds, cmap='tab10', alpha=0.7, s=30,\n",
    "                              edgecolors='black', linewidth=0.5)\n",
    "    axes[0].set_title('Our PyTorch K-Means Implementation', fontweight='bold', fontsize=14)\n",
    "    axes[0].set_xlabel('First Principal Component')\n",
    "    axes[0].set_ylabel('Second Principal Component')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter1, ax=axes[0])\n",
    "    \n",
    "    # Add cluster count\n",
    "    n_clusters_kmeans = len(np.unique(pytorch_preds))\n",
    "    axes[0].text(0.02, 0.98, f'Clusters: {n_clusters_kmeans}', \n",
    "                transform=axes[0].transAxes, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "                verticalalignment='top')\n",
    "    \n",
    "    # Stochastic Embedding\n",
    "    stochastic_preds = pytorch_trainer.predictions['stochastic_embedding']\n",
    "    scatter2 = axes[1].scatter(pca_features[:, 0], pca_features[:, 1], \n",
    "                              c=stochastic_preds, cmap='tab10', alpha=0.7, s=30,\n",
    "                              edgecolors='black', linewidth=0.5)\n",
    "    axes[1].set_title('Stochastic Embedding Network', fontweight='bold', fontsize=14)\n",
    "    axes[1].set_xlabel('First Principal Component')\n",
    "    axes[1].set_ylabel('Second Principal Component')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter2, ax=axes[1])\n",
    "    \n",
    "    # Add cluster count\n",
    "    n_clusters_stoch = len(np.unique(stochastic_preds))\n",
    "    axes[1].text(0.02, 0.98, f'Clusters: {n_clusters_stoch}', \n",
    "                transform=axes[1].transAxes, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "                verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pytorch_clustering_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Clustering comparison visualization saved!\")\n",
    "else:\n",
    "    print(\"âŒ Could not create visualization - missing model predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster quality with true labels (if available)\n",
    "if true_labels is not None:\n",
    "    print(\"=== CLUSTER ANALYSIS AGAINST TRUE LABELS ===\")\n",
    "    \n",
    "    label_names = label_encoder.classes_ if 'label_encoder' in locals() else None\n",
    "    \n",
    "    for model_name, predictions in pytorch_trainer.predictions.items():\n",
    "        print(f\"\\n--- {model_name.upper()} ---\")\n",
    "        \n",
    "        # Create confusion matrix-like analysis\n",
    "        cluster_composition = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for true_label, pred_label in zip(true_labels, predictions):\n",
    "            cluster_composition[pred_label][true_label] += 1\n",
    "        \n",
    "        # Print cluster composition\n",
    "        for cluster_id in sorted(cluster_composition.keys()):\n",
    "            total_in_cluster = sum(cluster_composition[cluster_id].values())\n",
    "            print(f\"\\nCluster {cluster_id} ({total_in_cluster} articles):\")\n",
    "            \n",
    "            for true_label, count in cluster_composition[cluster_id].items():\n",
    "                percentage = (count / total_in_cluster) * 100\n",
    "                label_name = label_names[true_label] if label_names is not None else f\"Label_{true_label}\"\n",
    "                print(f\"  {label_name}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Calculate cluster purity\n",
    "        total_articles = len(predictions)\n",
    "        correct_assignments = 0\n",
    "        \n",
    "        for cluster_id in cluster_composition:\n",
    "            if cluster_composition[cluster_id]:\n",
    "                max_count = max(cluster_composition[cluster_id].values())\n",
    "                correct_assignments += max_count\n",
    "        \n",
    "        purity = correct_assignments / total_articles\n",
    "        print(f\"\\nCluster Purity: {purity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1d620",
   "metadata": {},
   "source": [
    "## 7. Final Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00caf084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Performance Summary\n",
    "print(\"=== FINAL MODEL COMPARISON SUMMARY ===\")\n",
    "\n",
    "available_models = ['pytorch_kmeans', 'stochastic_embedding']\n",
    "models_trained = [m for m in available_models if m in pytorch_trainer.predictions]\n",
    "\n",
    "if models_trained:\n",
    "    print(f\"\\nâœ… Successfully trained {len(models_trained)} models:\")\n",
    "    for model in models_trained:\n",
    "        print(f\"   â€¢ {model.replace('_', ' ').title()}\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    if len(models_trained) >= 2 and 'silhouette_score' in eval_df.columns:\n",
    "        print(f\"\\nðŸ“Š Performance Rankings (by Silhouette Score):\")\n",
    "        sil_scores = eval_df.loc[models_trained, 'silhouette_score'].sort_values(ascending=False)\n",
    "        for i, (model, score) in enumerate(sil_scores.items(), 1):\n",
    "            print(f\"   {i}. {model.replace('_', ' ').title()}: {score:.4f}\")\n",
    "        \n",
    "        winner = sil_scores.index[0]\n",
    "        print(f\"\\nðŸ† Best performing model: {winner.replace('_', ' ').title()}\")\n",
    "else:\n",
    "    print(\"âŒ No models were successfully trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728c700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed32b8d4",
   "metadata": {},
   "source": [
    "## 8. Save Results and Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246244d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "results_data = {\n",
    "    'models': pytorch_trainer.models,\n",
    "    'predictions': pytorch_trainer.predictions,\n",
    "    'training_times': pytorch_trainer.training_times,\n",
    "    'evaluation_metrics': pytorch_trainer.evaluation_metrics,\n",
    "    'evaluation_dataframe': eval_df,\n",
    "    'config_used': config_choice,\n",
    "    'data_matrix_shape': data_matrix.shape\n",
    "}\n",
    "\n",
    "with open('model_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_data, f)\n",
    "\n",
    "# Save evaluation results as CSV\n",
    "eval_df.to_csv('model_evaluation_results.csv')\n",
    "\n",
    "print(\"âœ“ Saved model results to 'model_results.pkl'\")\n",
    "print(\"âœ“ Saved evaluation results to 'model_evaluation_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557fd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Project Summary\n",
    "print(\"=\"*80)\n",
    "print(\"   COMPARATIVE ANALYSIS: PYTORCH K-MEANS vs STOCHASTIC EMBEDDING\")\n",
    "print(\"              Neural Networks Course - Due Sept 14, 2025\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDATASET INFORMATION:\")\n",
    "print(f\"   - Application: News Article Clustering (BBC Dataset)\")\n",
    "print(f\"   - Total articles: {len(news_df)}\")\n",
    "print(f\"   - Feature configuration: {config_choice} TF-IDF\")\n",
    "print(f\"   - Feature matrix shape: {data_matrix.shape}\")\n",
    "if true_labels is not None:\n",
    "    print(f\"   - Ground truth categories: {len(np.unique(true_labels))}\")\n",
    "\n",
    "print(f\"\\nASSIGNMENT REQUIREMENTS FULFILLED:\")\n",
    "print(f\"   - Non-deterministic model: Stochastic Embedding Network\")\n",
    "print(f\"   - Stochastic formula: z = f(x) + Îµ where Îµ ~ N(0, ÏƒÂ²)\")\n",
    "print(f\"   - Reparameterization trick implementation\")\n",
    "print(f\"   - Uncertainty quantification methods\")\n",
    "print(f\"   - Multiple evaluation metrics (Silhouette, ARI, NMI)\")\n",
    "print(f\"   - Comparative analysis with deterministic baseline\")\n",
    "\n",
    "print(f\"\\nMODELS IMPLEMENTED:\")\n",
    "model_descriptions = {\n",
    "    'pytorch_kmeans': 'Custom PyTorch K-means implementation (primary model)',\n",
    "    'stochastic_embedding': 'Assignment-specified stochastic embedding network'\n",
    "}\n",
    "\n",
    "for i, (model_name, description) in enumerate(model_descriptions.items(), 1):\n",
    "    if model_name in pytorch_trainer.models:\n",
    "        print(f\"   {i}. {description}\")\n",
    "\n",
    "print(f\"\\nPERFORMANCE EVALUATION:\")\n",
    "if 'silhouette_score' in eval_df.columns:\n",
    "    best_silhouette = eval_df['silhouette_score'].idxmax()\n",
    "    best_sil_score = eval_df.loc[best_silhouette, 'silhouette_score']\n",
    "    print(f\"   - Best Silhouette Score: {best_silhouette.replace('_', ' ').title()} ({best_sil_score:.4f})\")\n",
    "\n",
    "if true_labels is not None and 'adjusted_rand_score' in eval_df.columns:\n",
    "    best_ari = eval_df['adjusted_rand_score'].idxmax()\n",
    "    best_ari_score = eval_df.loc[best_ari, 'adjusted_rand_score']\n",
    "    print(f\"   - Best Adjusted Rand Index: {best_ari.replace('_', ' ').title()} ({best_ari_score:.4f})\")\n",
    "\n",
    "if 'normalized_mutual_info_score' in eval_df.columns:\n",
    "    best_nmi = eval_df['normalized_mutual_info_score'].idxmax()\n",
    "    best_nmi_score = eval_df.loc[best_nmi, 'normalized_mutual_info_score']\n",
    "    print(f\"   - Best Normalized Mutual Info: {best_nmi.replace('_', ' ').title()} ({best_nmi_score:.4f})\")\n",
    "\n",
    "# Compare our two models\n",
    "available_models = ['pytorch_kmeans', 'stochastic_embedding']\n",
    "trained_models = [m for m in available_models if m in eval_df.index]\n",
    "\n",
    "if len(trained_models) == 2 and 'silhouette_score' in eval_df.columns:\n",
    "    kmeans_score = eval_df.loc['pytorch_kmeans', 'silhouette_score']\n",
    "    stoch_score = eval_df.loc['stochastic_embedding', 'silhouette_score']\n",
    "    print(f\"\\nDIRECT MODEL COMPARISON:\")\n",
    "    print(f\"   - PyTorch K-means: {kmeans_score:.4f}\")\n",
    "    print(f\"   - Stochastic Embedding: {stoch_score:.4f}\")\n",
    "    \n",
    "    if kmeans_score > stoch_score:\n",
    "        improvement = ((kmeans_score - stoch_score) / stoch_score) * 100\n",
    "        print(f\"   - K-means performs {improvement:.1f}% better\")\n",
    "    else:\n",
    "        improvement = ((stoch_score - kmeans_score) / kmeans_score) * 100\n",
    "        print(f\"   - Stochastic method performs {improvement:.1f}% better\")\n",
    "\n",
    "print(f\"\\nUNCERTAINTY ANALYSIS:\")\n",
    "if uncertainty_analysis:\n",
    "    for model_name, metrics in uncertainty_analysis.items():\n",
    "        readable_name = model_name.replace('_', ' ').title()\n",
    "        print(f\"   - {readable_name}: Mean uncertainty = {metrics['mean_uncertainty']:.4f}\")\n",
    "\n",
    "if stability_results:\n",
    "    print(f\"\\nSTABILITY ANALYSIS:\")\n",
    "    for model_name, metrics in stability_results.items():\n",
    "        readable_name = model_name.replace('_', ' ').title()\n",
    "        print(f\"   - {readable_name}: Consistency = {metrics['mean_ari']:.4f} ARI\")\n",
    "\n",
    "print(f\"\\nKEY INSIGHTS:\")\n",
    "print(f\"   - PyTorch K-means provides efficient deterministic clustering\")\n",
    "print(f\"   - Stochastic embedding enables uncertainty quantification\")\n",
    "print(f\"   - Reparameterization trick allows gradient-based stochastic training\")\n",
    "print(f\"   - Input-dependent noise modeling adapts to local data structure\")\n",
    "print(f\"   - Trade-off exists between clustering performance and uncertainty estimation\")\n",
    "\n",
    "print(f\"\\nPROJECT DELIVERABLES:\")\n",
    "print(f\"   - model_results.pkl - Complete trained models and results\")\n",
    "print(f\"   - model_evaluation_results.csv - Quantitative evaluation metrics\")\n",
    "print(f\"   - pytorch_model_comparison.png - Performance comparison visualizations\")\n",
    "print(f\"   - pytorch_clustering_visualization.png - Side-by-side cluster comparisons\")\n",
    "\n",
    "print(f\"\\nPROJECT STATUS:\")\n",
    "print(f\"   - PyTorch K-means: IMPLEMENTED AND TRAINED\")\n",
    "print(f\"   - Stochastic Embedding: IMPLEMENTED AND TRAINED\") \n",
    "print(f\"   - Comparative Analysis: COMPLETE\")\n",
    "print(f\"   - Evaluation & Visualization: COMPLETE\")\n",
    "print(f\"   - Assignment Requirements: FULFILLED\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARATIVE CLUSTERING ANALYSIS PROJECT COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
